{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  Dataset\n",
    "import polars \n",
    "import torch\n",
    "from streaming.base import MDSWriter\n",
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = polars.read_parquet(\"data/train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"symbol_id\", \"time_id\"] +  [f\"feature_{idx:02d}\" for idx in range(79)]+ [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "\n",
    "#select target values\n",
    "target_cols = [\"responder_6\"]\n",
    "\n",
    "# select the weight values\n",
    "weight_cols = [\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, accerlerator):\n",
    "        # Store each part of the dataframe that needed as a tensor\n",
    "        self.features = torch.FloatTensor(dataframe.select(feature_cols).to_numpy()).to(accerlerator)\n",
    "        self.labels = torch.FloatTensor(dataframe.select(target_cols).to_numpy()).to(accerlerator)\n",
    "        self.weights = torch.FloatTensor(dataframe.select(weight_cols).to_numpy()).to(accerlerator)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returb the length of the dataset\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return the data at a given index\n",
    "        # x = all features defined in the feature_cols\n",
    "        # y = the target values that needed to be predicted definded in the target_cols\n",
    "        # w = the weight values for calculating the loss defined in the weight_cols\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        w = self.weights[idx]\n",
    "        return x, y, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'data_for_nn/'\n",
    "columns = {'x': 'pkl', 'y': 'pkl', 'w':'pkl'}\n",
    "compression = 'zstd:7' # compress shards with ZStandard, level 7\n",
    "# Here we use a human-readable string, but we could also\n",
    "# pass in an int specifying the number of bytes.\n",
    "limit = '100kb'\n",
    "hashes = ['sha1'] # Use only SHA1 hashing on each shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(train_df, accerlerator=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MDSWriter(out=output_dir, columns=columns, compression=compression, hashes=hashes, size_limit=limit) as out:\n",
    "    for x, y , w in dataset:\n",
    "        out.write({'x': x, 'y': y, \"w\":w })\n",
    "        # raise error i think due to the data is too big"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
